{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP+Eb0JEi0iMaxd0HZUastL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/meirbelo/gapevine_dataset/blob/main/new_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.models import Model\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "# Data augmentation pour l'ensemble d'entraînement\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    vertical_flip=True\n",
        ")\n",
        "\n",
        "# Créer le générateur pour l'ensemble d'entraînement\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    directory=\"content/new_dataset/train\",\n",
        "    target_size=(224, 224),\n",
        "    color_mode=\"rgb\",\n",
        "    batch_size=32,\n",
        "    class_mode=\"binary\",\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "# Data augmentation pour l'ensemble de validation\n",
        "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "    directory=\"content/new_dataset/validation\",\n",
        "    target_size=(224, 224),\n",
        "    color_mode=\"rgb\",\n",
        "    batch_size=32,\n",
        "    class_mode=\"binary\",\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "# Data augmentation pour l'ensemble de test\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    directory=\"content/new_dataset/test\",\n",
        "    target_size=(224, 224),\n",
        "    color_mode=\"rgb\",\n",
        "    batch_size=32,\n",
        "    class_mode=\"binary\",\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "# Charger le modèle VGG16 pré-entraîné sur ImageNet et sans la dernière couche de classification\n",
        "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "# Ajouter une nouvelle couche de classification\n",
        "x = base_model.output\n",
        "x = Flatten()(x)\n",
        "x = Dense(512, activation='relu')(x)\n",
        "x = Dropout(0.2)(x)\n",
        "predictions = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "# Définir le modèle final\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "# Congeler les couches du modèle de base pour ne pas les entraîner\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "model.compile(optimizer=Adam(learning_rate=0.001, decay=1e-5),\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "# Entraîner le modèle avec les générateurs d'images\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=train_generator.samples // train_generator.batch_size,\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=validation_generator.samples // validation_generator.batch_size,\n",
        "    epochs=10\n",
        ")\n",
        "\n",
        "# Évaluer le modèle sur l'ensemble de test\n",
        "scores = model.evaluate(\n",
        "    test_generator,\n",
        "    steps=test_generator.samples // test_generator.batch_size\n",
        ")\n",
        "\n",
        "print('Test loss:', scores[0])\n",
        "print('Test accuracy:', scores[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oWgvbPN30zJ8",
        "outputId": "e5226ad8-72a9-47a5-f09e-7898f794420c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 7000 images belonging to 2 classes.\n",
            "Found 1500 images belonging to 2 classes.\n",
            "Found 1500 images belonging to 2 classes.\n",
            "Epoch 1/10\n",
            "218/218 [==============================] - 107s 487ms/step - loss: 0.2305 - accuracy: 0.9400 - val_loss: 0.0220 - val_accuracy: 0.9905\n",
            "Epoch 2/10\n",
            "218/218 [==============================] - 110s 506ms/step - loss: 0.0300 - accuracy: 0.9911 - val_loss: 0.0045 - val_accuracy: 0.9986\n",
            "Epoch 3/10\n",
            "218/218 [==============================] - 110s 504ms/step - loss: 0.0303 - accuracy: 0.9894 - val_loss: 0.0185 - val_accuracy: 0.9912\n",
            "Epoch 4/10\n",
            " 84/218 [==========>...................] - ETA: 1:00 - loss: 0.0262 - accuracy: 0.9892"
          ]
        }
      ]
    }
  ]
}